# Document Compliance PoC Toolkit

A lightweight toolkit for rapidly building AI-powered document compliance proof-of-concepts. Leverage multiple similarity-analysis techniques to verify that legal, financial or other documents include all required clausesâ€”and expose them via a simple REST API.

Here is a more detailed [explanation](document_analysis.md).

> note - this repo is in active development. api and ui are not operational

## Key Features

- **Clause Verification**  
  Detect and score the presence of required clauses in any document.

- **Multi-Technique Similarity**  
  Compare documents using TF-IDF, cosine similarity over embeddings, and more.

- **Modular Architecture**  
  Swap in your preferred NLP models or similarity algorithms with minimal changes.

- **Extensible Examples**  
  Sample configs and test documents to help you get started in minutes.

## Overview

The compare notebook implements multiple methods to measure text similarity:

- **LLM-based similarity**: Uses Azure OpenAI's GPT models to semantically compare clauses.
- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weights terms based on their importance within the text corpus.
- **Cosine Similarity**: Measures semantic closeness between text embeddings generated by transformer models.

The proccess notebook needs to be modify to extract the actual data needed for your documents. To do this simply create an new analyzer schema to extract the needed data

## Prerequisites

- Python 3.8 or higher
- Azure subscription with access to Azure OpenAI and Azure AI Services
- Required Python libraries (see below)

## Installation

1. **Clone the repository**

```bash
git clone <your-repo-url>
cd <your-repo-directory>
```

2. **Create a virtual environment (recommended)**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

### Example `requirements.txt`

```
python-dotenv
numpy
tensorflow
sentence-transformers
scikit-learn
pandas
openai>=1.0.0
PyMuPDF
```

## Azure Configuration

Create a `.env` file in the root directory and add your Azure credentials:

```env
AZURE_AI_SERVICES_ENDPOINT=<your-azure-ai-services-endpoint>
AZURE_AI_SERVICES_API_VERSION=<your-azure-ai-services-api-version>
AZURE_AI_SERVICES_API_KEY=<your-azure-ai-services-api-key>

AZURE_OPENAI_ENDPOINT=<your-azure-openai-endpoint>
AZURE_OPENAI_API_KEY=<your-azure-openai-api-key>
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=<your-chat-deployment-name>
AZURE_OPENAI_CHAT_API_VERSION=<your-chat-api-version>
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME=<your-embeddings-deployment-name>
AZURE_OPENAI_EMBEDDINGS_API_VERSION=<your-embeddings-api-version>
```

## Data Preparation

The notebook expects two JSONL files:

- `master_contract_clause.jsonl`: Contains the required clauses (golden clauses).
- `chunks.jsonl`: Contains the document chunks to be checked.

Each JSONL file should have one JSON object per line, formatted as:

```json
{"clauseName": "Clause Name", "clauseText": "Clause text here."}
```

or for chunks:

```json
{"text": "Chunk text here."}
```

You can generate these files using the provided PDF extraction scripts in the notebook.

## Running the Notebook

Launch Jupyter Notebook:

```bash
jupyter notebook
```

Open `compare.ipynb` and run the cells sequentially.

## Notebook Structure

- **Environment Setup**: Loads environment variables and validates Azure configurations.
- **Data Loading**: Functions to load JSONL data.
- **Similarity Checks**:
  - **LLM-based similarity**: Uses Azure OpenAI GPT models to compare clauses.
  - **Embedding-based similarity**: Uses SentenceTransformer embeddings and computes cosine similarity.
  - **TF-IDF similarity**: Computes similarity based on term importance.
- **Results Display**: Presents similarity results in a clear Pandas DataFrame.

## Troubleshooting

- **OpenAI SDK Errors**: If you encounter errors related to the OpenAI SDK, ensure you're using the latest version (`openai>=1.0.0`) and replace deprecated parameters (`engine`) with the new `model` parameter.

## Best Practices

- Always store sensitive credentials securely using environment variables.
- Regularly update dependencies to their latest stable versions.
- Follow Azure best practices for security and resource management.

## License

This project is licensed under the MIT License.