{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides several techniques for comparing the simlarity between documents \n",
    "\n",
    "- LLM as compared the semantic similarity between chunck of text\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency) weights terms based on how uniquely important they are in your text, helping to prioritize meaningful words.\n",
    "- Cosine Similarity measures the angle between vectors (created by TF-IDF), capturing semantic closeness between clauses even if wording slightly differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "load_dotenv(override=True)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICES_ENDPOINT = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_AI_SERVICES_API_VERSION = os.getenv(\"AZURE_AI_SERVICES_API_VERSION\")\n",
    "AZURE_AI_SERVICES_API_KEY = os.getenv(\"AZURE_AI_SERVICES_API_KEY\", None)\n",
    "AZURE_AI_DOCUMENT_ENDPOINT = os.getenv(\"AZURE_AI_DOCUMENT_ENDPOINT\") or os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_AI_DOCUMENT_API_KEY = os.getenv(\"AZURE_AI_DOCUMENT_API_KEY\", None)\n",
    "assert AZURE_AI_SERVICES_ENDPOINT, \"AZURE_AI_SERVICES_ENDPOINT must be set\"\n",
    "assert AZURE_AI_SERVICES_API_VERSION, \"AZURE_AI_SERVICES_API_VERSION must be set\"\n",
    "assert AZURE_AI_DOCUMENT_ENDPOINT, \"AZURE_AI_DOCUMENT_ENDPOINT must be set\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", None)\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\")\n",
    "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDINGS_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\")\n",
    "assert AZURE_OPENAI_ENDPOINT, \"AZURE_OPENAI_ENDPOINT must be set\"\n",
    "assert (AZURE_OPENAI_CHAT_DEPLOYMENT_NAME), \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME must be set\"\n",
    "assert (AZURE_OPENAI_CHAT_API_VERSION), \"AZURE_OPENAI_CHAT_API_VERSION must be set\"\n",
    "assert (AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME), \"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME must be set\"\n",
    "assert (AZURE_OPENAI_EMBEDDINGS_API_VERSION), \"AZURE_OPENAI_EMBEDDINGS_API_VERSION must be set\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_items(jsonl_path):\n",
    "    items = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            items.append(data)\n",
    "    return items\n",
    "\n",
    "def display_results_as_dataframe(results):\n",
    "    if not results:\n",
    "        display(Markdown(\"**No results to display.**\"))\n",
    "        return\n",
    "\n",
    "    data = []\n",
    "    for clause_name, info in results.items():\n",
    "        data.append({\n",
    "            \"Clause Name\": clause_name,\n",
    "            \"Similarity\": f\"{info['score']:.2f}\",\n",
    "            \"Best Chunk Index\": info[\"chunk_index\"],\n",
    "            \"Chunk Text\": info[\"chunk_text\"]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data).sort_values(by=\"Similarity\", ascending=False)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "    api_key=AZURE_OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def load_items(jsonl_file):\n",
    "    items = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            items.append(data)\n",
    "    return items\n",
    "\n",
    "def check_clauses(contract_chunks, required_clauses):\n",
    "    clause_similarities = {}\n",
    "\n",
    "    for clause in required_clauses:\n",
    "        clause_name = clause.get(\"clauseName\", \"Unknown Clause\")\n",
    "        clause_text = clause.get(\"clauseText\", \"\")\n",
    "        best_score = 0\n",
    "        best_chunk_index = -1\n",
    "        best_chunk_text = \"\"\n",
    "\n",
    "        for idx, chunk in enumerate(contract_chunks):\n",
    "            chunk_text = chunk.get(\"text\", \"\")\n",
    "            prompt = (\n",
    "                \"You are a legal assistant tasked with ensuring legal documents contain the proper clauses. \"\n",
    "                \"You will be given two clauses to compare. Your job is to determine if the two clauses express the same intent. \"\n",
    "                f\"Golden Clause: '{clause_text}'\\nDocument Clause: '{chunk_text}'\\n\"\n",
    "                \"Respond with a value from 0 to 1, where 0 means the clauses are not similar and 1 means they are similar. \"\n",
    "                \"If you are not sure, respond with 0.\\n\"\n",
    "                \"Please respond with a single number, without any additional text.\"\n",
    "            )\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=10\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content.strip()\n",
    "\n",
    "            try:\n",
    "                score = float(reply)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_chunk_index = idx\n",
    "                    best_chunk_text = chunk_text\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        clause_similarities[clause_name] = {\n",
    "            \"score\": best_score,\n",
    "            \"chunk_index\": best_chunk_index,\n",
    "            \"chunk_text\": best_chunk_text\n",
    "        }\n",
    "\n",
    "    return clause_similarities\n",
    "\n",
    "master_clauses = load_items(\"master_contract_clause.jsonl\")\n",
    "sample_chunks  = load_items(\"chunks.jsonl\")\n",
    "llm_results = check_clauses(sample_chunks, master_clauses)\n",
    "\n",
    "display_results_as_dataframe(llm_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def embed_text(items, model):\n",
    "    if not items:\n",
    "        return np.array([])\n",
    "    return model.encode(items, show_progress_bar=False)\n",
    "\n",
    "def compute_tfidf_similarity(clause_texts, chunk_texts):\n",
    "    if not clause_texts or not chunk_texts:\n",
    "        return np.array([])\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(clause_texts + chunk_texts)\n",
    "\n",
    "    clause_tfidf = tfidf_matrix[:len(clause_texts)]\n",
    "    chunk_tfidf = tfidf_matrix[len(clause_texts):]\n",
    "\n",
    "    similarity_matrix = clause_tfidf * chunk_tfidf.T\n",
    "    similarity_matrix = similarity_matrix.toarray()\n",
    "\n",
    "    if similarity_matrix.size > 0 and similarity_matrix.max() != similarity_matrix.min():\n",
    "        similarity_matrix = (\n",
    "            (similarity_matrix - similarity_matrix.min())\n",
    "            / (similarity_matrix.max() - similarity_matrix.min())\n",
    "        )\n",
    "    return similarity_matrix\n",
    "\n",
    "def check_clauses(contract_chunks, required_clauses, model, alpha=0.7, beta=0.3):\n",
    "    clause_texts = [c[\"clauseText\"].strip().lower() for c in required_clauses if \"clauseText\" in c and c[\"clauseText\"].strip()]\n",
    "    chunk_texts  = [ch[\"text\"].strip().lower() for ch in contract_chunks if \"text\" in ch and ch[\"text\"].strip()]\n",
    "\n",
    "    if not clause_texts or not chunk_texts:\n",
    "        return {}\n",
    "\n",
    "    chunk_embeddings = embed_text(chunk_texts, model)\n",
    "    clause_embeddings = embed_text(clause_texts, model)\n",
    "\n",
    "    tfidf_similarities = compute_tfidf_similarity(clause_texts, chunk_texts)\n",
    "\n",
    "    clause_similarities = {}\n",
    "\n",
    "    for i, clause_emb in enumerate(clause_embeddings):\n",
    "        best_score = -1\n",
    "        best_chunk_index = None\n",
    "\n",
    "        for j, chunk_emb in enumerate(chunk_embeddings):\n",
    "            sim = tf.keras.losses.cosine_similarity(clause_emb, chunk_emb).numpy()\n",
    "            sim = -sim\n",
    "            norm_sim = (sim + 1) / 2\n",
    "\n",
    "            tfidf_part = tfidf_similarities[i, j] if tfidf_similarities.size else 0\n",
    "\n",
    "            combined_score = (alpha * norm_sim + beta * tfidf_part)\n",
    "\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_chunk_index = j\n",
    "\n",
    "        clause_name = required_clauses[i].get(\"clauseName\", f\"Clause_{i}\").strip()\n",
    "        chunk_text = chunk_texts[best_chunk_index] if best_chunk_index is not None else None\n",
    "\n",
    "        clause_similarities[clause_name] = {\n",
    "            \"score\": best_score,\n",
    "            \"chunk_index\": best_chunk_index,\n",
    "            \"chunk_text\": chunk_text\n",
    "        }\n",
    "\n",
    "    return clause_similarities\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "master_clauses = load_items(\"master_contract_clause.jsonl\")\n",
    "sample_chunks  = load_items(\"chunks.jsonl\")\n",
    "\n",
    "sim_results = check_clauses(\n",
    "    contract_chunks=sample_chunks,\n",
    "    required_clauses=master_clauses,\n",
    "    model=model,\n",
    "    alpha=0.7,\n",
    "    beta=0.3\n",
    ")\n",
    "\n",
    "display_results_as_dataframe(sim_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_similarity_report(llm_results, sim_results):\n",
    "    report = []\n",
    "\n",
    "    for clause_name in llm_results.keys():\n",
    "        llm_score = llm_results[clause_name]['score']\n",
    "        semantic_score = sim_results.get(clause_name, {}).get('score', 0)\n",
    "\n",
    "        prompt = (\n",
    "            \"You are a legal assistant tasked with generating a concise similarity report for legal clauses. \"\n",
    "            f\"For the clause '{clause_name}', the LLM semantic similarity score is {llm_score:.2f}, \"\n",
    "            f\"and the combined TF-IDF and cosine similarity score is {semantic_score:.2f}. \"\n",
    "            \"Provide a brief summary (1-2 sentences) explaining the similarity between the golden clause and the document clause, \"\n",
    "            \"taking into account both scores. Clearly state if the clauses are sufficiently similar or if further review is recommended.\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "\n",
    "        report.append({\n",
    "            \"Clause Name\": clause_name,\n",
    "            \"LLM Similarity\": f\"{llm_score:.2f}\",\n",
    "            \"TF-IDF & Cosine Similarity\": f\"{semantic_score:.2f}\",\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "\n",
    "    df_report = pd.DataFrame(report)\n",
    "    display(df_report)\n",
    "\n",
    "generate_similarity_report(llm_results, sim_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
